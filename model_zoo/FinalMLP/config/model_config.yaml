Base:
    model_root: './checkpoints/'
    num_workers: 3
    verbose: 1
    early_stop_patience: 2
    pickle_feature_encoder: True
    save_best_only: True
    eval_steps: null
    debug_mode: False
    group_id: null
    use_features: null
    feature_specs: null
    feature_config: null

DualMLP_test:
    model: DualMLP
    dataset_id: tiny_h5
    loss: binary_crossentropy
    metrics: ['logloss', 'AUC']
    task: binary_classification
    optimizer: adam
    learning_rate: 1.e-3
    embedding_regularizer: 1.e-8
    net_regularizer: 0
    mlp1_hidden_units: [64, 32]
    mlp1_hidden_activations: relu
    mlp1_dropout: 0
    mlp1_batch_norm: False
    mlp2_hidden_units: [64, 64, 64]
    mlp2_hidden_activations: relu
    mlp2_dropout: 0
    mlp2_batch_norm: False
    batch_size: 128
    embedding_dim: 4
    epochs: 100
    shuffle: True
    seed: 2019
    monitor: 'AUC'
    monitor_mode: 'max'

DualMLP_default: # This is a config template
    model: DualMLP
    dataset_id: TBD
    loss: binary_crossentropy
    metrics: ['logloss', 'AUC']
    task: binary_classification
    optimizer: adam
    learning_rate: 1.e-3
    embedding_regularizer: 0
    net_regularizer: 0
    mlp1_hidden_units: [64, 32]
    mlp1_hidden_activations: relu
    mlp1_dropout: 0
    mlp1_batch_norm: False
    mlp2_hidden_units: [64, 64, 64]
    mlp2_hidden_activations: relu
    mlp2_dropout: 0
    mlp2_batch_norm: False
    batch_size: 10000
    embedding_dim: 16
    epochs: 100
    shuffle: True
    seed: 20222023
    monitor: {'AUC': 1, 'logloss': -1}
    monitor_mode: 'max'

FinalMLP_test:
    model: FinalMLP
    dataset_id: tiny_h5
    loss: binary_crossentropy
    metrics: ['logloss', 'AUC']
    task: binary_classification
    optimizer: adam
    learning_rate: 1.e-3
    embedding_regularizer: 0
    net_regularizer: 0
    mlp1_hidden_units: [64, 32]
    mlp1_hidden_activations: relu
    mlp1_dropout: 0
    mlp1_batch_norm: False
    mlp2_hidden_units: [64, 64, 64]
    mlp2_hidden_activations: relu
    mlp2_dropout: 0
    mlp2_batch_norm: False
    use_fs: True
    fs_hidden_units: [64, 64]
    fs1_context: ["userid"]
    fs2_context: ["adgroup_id", "cate_id"]
    num_heads: 2
    batch_size: 128
    embedding_dim: 4
    epochs: 100
    shuffle: True
    seed: 2019
    monitor: 'AUC'
    monitor_mode: 'max'

DualMLP_eleme: # This is a config template
    model: DualMLP
    dataset_id: eleme
    loss: binary_crossentropy
    metrics: ['logloss', 'AUC', 'precision', 'recall', 'f1']
    task: binary_classification
    optimizer: adam
    learning_rate: 1.e-3
    embedding_regularizer: 0.05
    net_regularizer: 0
    mlp1_hidden_units: [400]
    mlp1_hidden_activations: relu
    mlp1_dropout: 0.2
    mlp1_batch_norm: True
    mlp2_hidden_units: [800]
    mlp2_hidden_activations: relu
    mlp2_dropout: 0.1
    mlp2_batch_norm: True
    use_fs: False
    fs_hidden_units: [800]
    fs1_context: []
    fs2_context: []
    num_heads: 10
    batch_size: 4096
    embedding_dim: 32
    epochs: 100
    shuffle: True
    seed: 2023
    monitor: {'AUC': 1, 'logloss': 0}
    monitor_mode: 'max'
    save_best_only: True


DualMLP_bundle: # This is a config template
    model: DualMLP
    dataset_id: bundle
    loss: binary_crossentropy
    metrics: ['logloss', 'AUC', 'precision', 'recall', 'f1']
    task: binary_classification
    optimizer: adam
    learning_rate: 1.e-3
    embedding_regularizer: 0.05
    net_regularizer: 0
    mlp1_hidden_units: [400]
    mlp1_hidden_activations: relu
    mlp1_dropout: 0.2
    mlp1_batch_norm: True
    mlp2_hidden_units: [800]
    mlp2_hidden_activations: relu
    mlp2_dropout: 0.1
    mlp2_batch_norm: True
    use_fs: False
    fs_hidden_units: [800]
    fs1_context: []
    fs2_context: []
    num_heads: 10
    batch_size: 4096
    embedding_dim: 10
    epochs: 100
    shuffle: True
    seed: 2023
    monitor: {'AUC': 1, 'logloss': 0}
    monitor_mode: 'max'
    save_best_only: True


DualMLP_ml10m: # This is a config template
    model: DualMLP
    dataset_id: ml_10m
    loss: binary_crossentropy
    metrics: ['logloss', 'AUC', 'precision', 'recall', 'f1']
    task: binary_classification
    optimizer: adam
    learning_rate: 1.e-3
    embedding_regularizer: 0.01
    net_regularizer: 0
    mlp1_hidden_units: [400]
    mlp1_hidden_activations: relu
    mlp1_dropout: 0.4
    mlp1_batch_norm: True
    mlp2_hidden_units: [800]
    mlp2_hidden_activations: relu
    mlp2_dropout: 0.2
    mlp2_batch_norm: True
    use_fs: False
    fs_hidden_units: [800]
    fs1_context: []
    fs2_context: []
    num_heads: 10
    batch_size: 4096
    embedding_dim: 10
    epochs: 100
    shuffle: True
    seed: 2023
    monitor: {'AUC': 1, 'logloss': 0}
    monitor_mode: 'max'
    save_best_only: True

FinalMLP_eleme: # This is a config template
    model: FinalMLP
    dataset_id: eleme
    loss: binary_crossentropy
    metrics: ['logloss', 'AUC', 'precision', 'recall', 'f1']
    task: binary_classification
    optimizer: adam
    learning_rate: 1.e-3
    embedding_regularizer: 0.05
    net_regularizer: 0
    mlp1_hidden_units: [400]
    mlp1_hidden_activations: relu
    mlp1_dropout: 0.2
    mlp1_batch_norm: True
    mlp2_hidden_units: [800]
    mlp2_hidden_activations: relu
    mlp2_dropout: 0.1
    mlp2_batch_norm: True
    use_fs: True
    fs_hidden_units: [800]
    fs1_context: []
    fs2_context: []
    num_heads: 10
    batch_size: 4096
    embedding_dim: 32
    epochs: 100
    shuffle: True
    seed: 2023
    monitor: {'AUC': 1, 'logloss': 0}
    monitor_mode: 'max'
    save_best_only: True

FinalMLP_bundle: # This is a config template
    model: FinalMLP
    dataset_id: bundle
    loss: binary_crossentropy
    metrics: ['logloss', 'AUC', 'precision', 'recall', 'f1']
    task: binary_classification
    optimizer: adam
    learning_rate: 1.e-3
    embedding_regularizer: 0.05
    net_regularizer: 0
    mlp1_hidden_units: [400]
    mlp1_hidden_activations: relu
    mlp1_dropout: 0.2
    mlp1_batch_norm: True
    mlp2_hidden_units: [800]
    mlp2_hidden_activations: relu
    mlp2_dropout: 0.1
    mlp2_batch_norm: True
    use_fs: True
    fs_hidden_units: [800]
    fs1_context: []
    fs2_context: []
    num_heads: 10
    batch_size: 4096
    embedding_dim: 10
    epochs: 100
    shuffle: True
    seed: 2023
    monitor: {'AUC': 1, 'logloss': 0}
    monitor_mode: 'max'
    save_best_only: True

FinalMLP_ml10m: # This is a config template
    model: FinalMLP
    dataset_id: ml_10m
    loss: binary_crossentropy
    metrics: ['logloss', 'AUC', 'precision', 'recall', 'f1']
    task: binary_classification
    optimizer: adam
    learning_rate: 1.e-3
    embedding_regularizer: 0.01
    net_regularizer: 0
    mlp1_hidden_units: [400]
    mlp1_hidden_activations: relu
    mlp1_dropout: 0.4
    mlp1_batch_norm: True
    mlp2_hidden_units: [800]
    mlp2_hidden_activations: relu
    mlp2_dropout: 0.2
    mlp2_batch_norm: True
    use_fs: True
    fs_hidden_units: [800]
    fs1_context: []
    fs2_context: []
    num_heads: 10
    batch_size: 4096
    embedding_dim: 10
    epochs: 100
    shuffle: True
    seed: 2023
    monitor: {'AUC': 1, 'logloss': 0}
    monitor_mode: 'max'
    save_best_only: True